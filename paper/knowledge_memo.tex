\textit{The following memo details revisions I have made to my manuscript in light of the comments and suggestions I received from three reviewers.  I thank each of the reviewers for their support of the project and the comments/suggestions provided. I have addressed each of their comments directly in the revised manuscript and/or by providing additional analyses in the appendix.  For instance, I have done XXX. I believe the comments and suggestions have helped us produce a stronger revised manuscript and hope that the reviewers and editor agree.}

\subsection*{Reviewer 1}

This is a promising paper. In it, the author makes a convincing case that a key aspect of citizen sophistication, discursive sophistication, can be measured via automated text analysis of responses to open-ended survey items, and that the resulting measures 1) are only modestly correlated with conventional civics knowledge scales, 2) generally exert stronger influence on criterion variables than does civics knowledge, and 3) show no sign of the gender gap we typically observe on knowledge measures. This is a lot, especially given that analyses make use of data from multiple surveys. The paper is very interesting and largely persuasive. I see it as having good prospects for eventual publication.

My two concerns are somewhat broad, and each relates to aspects of how the paper is framed and what it ultimately accomplishes. Neither of these should be terribly difficult for the author to address.

First, the juxtaposition between the new measure and civics knowledge measures is overdone. At several points, the paper is framed as if to suggest that there is an implied competition between the new measure and knowledge scales. But there is no such competition either conceptually or empirically. Conceptually, there is no reason that political sophistication cannot include both an analytical, integrative component (as represented by the current paper's text-based measures) and an information raw material component (as represented by civics knowledge). Empirically, we see in the current paper's Figure 2 that the two are only modestly correlated, and in Figure 3 that both produce effects on typical political dependent variables (although it does not appear that both variables are include as IVs at the same time; they should be, at least as one specification in the appendix models). This all suggests that the two variables are complements, not competitors. To me, this is a
better way to frame the paper: political scientists have obsessed about knowledge, but there is more to sophistication than knowledge; we can add a good measure of a different aspect of sophistication through automated text coding, and doing so deepens our understanding of the importance of sophistication, and does so in a manner that suggests an absence of a gender gap. 

It also is possible that the text measure and the civics measure operate in conjunction with one another. I recommend exploring whether they interact. If they do, two different patterns seem sensible. First, there could be weak or null main effects, but a significant positive interaction, in models with the political DVs. This would suggest that knowledge and discursive sophistication are individually insufficient to produce positive effects, but that those effects are achieved in combination. In other words, we get the most civic engagement when a person has a good baseline factual understanding of politics along with the cognitive sophistication to connect the dots and form cohesive, well-thought opinions. Second, and very different, there could be positive main effects, but a significant negative interaction. This would suggest that either knowledge or discursive sophistication is sufficient in itself to produce good behavioral effects, but that there is a diminishing
return when they operate in combination. These tests, in turn, could be broken out by gender. That could be very valuable to explore, because it potentially could show that discursive sophistication not only does not exhibit a gender gap, but that it acts to reduce the effects of the standard gender gap in knowledge.

Second, there is room to do a more thorough job of differentiating between discursive sophistication as a general phenomenon and discursive political sophistication. This is mentioned briefly in the conclusion, but more discussion would be good. This issue harkens back to Nie et al.'s The Changing American Voter and Eric Smith's The Unchanging American Voter, with Smith showing that the supposed change observed by Nie et al. mostly involved verbosity, or general communication skill, rather than anything that was specifically political. The current paper has an appendix table in which discursive sophistication is regressed on some demographic variables. This is a start, but I would recommend 1) when discursive sophistication is the DV, adding a fuller representation of education as an IV, and including whatever measure of verbal skill is available, along with anything else, such as cognitive ability, that the surveys might include. In fact, civics knowledge also could be
included. That way, the models would show how much of discursive sophistication is and is not accounted for by conventional predictors (the multivariate models with discursive sophistication as an IV also should include as covariates anything that is associated with both discursive sophistication and the DV), and 2) addressing this at the conceptual level in the paper. Does the discursive measure merely show that some people are better than others at formulating and communicating arguments in general, or is there something distinctly political about it? This, of course, links to my first point about sophistication vs. knowledge: it potentially is the case that the author's measure captures and important general aspect of sophistication, whereas civics knowledge contributes a political dimension.

Some minor points:

1.	I would change the first part of the title. Discursive sophistication is not about "knowing" in the conventional sense, it is about sophistication in ability. Changing the title would help with decoupling discursive sophistication from knowledge, thereby improving the paper's framing.
2.	Luskin's 1990 Political Behavior paper is worth a look.
3.	It is Delli Carpini, not Carpini. "Delli" is part of the last name, not a middle name.
4.	Figure 1 is not necessary.
5.	I'm pretty sure Mondak (2001) was the first to make the point about coding problems on open-ended knowledge items (that's what is noted in Gibson and Caldeira 2009).


\subsection*{Reviewer 2}

This manuscript proposes a novel way to measure the complex and polysemic concept of political sophistication. Rather than using typical survey items measuring factual knowledge focusing on the accretion of electoral and partisan facts (and with closed ended format implying the selection of the correct answer among a list of potential answers), the paper proposes a measure of what it calls: "discursive sophistication" based on how people discuss their political preferences in open ended survey responses. 

The piece is well written, and the findings are interesting. Moreover, I recognize that the authors have done an excellent job of situating their project in the literature and of making a case for their contribution. I am also impressed by the amount of empirical work presented in such an elegant way. However, in my view at present the contribution is methodological. In what follows I provide a number of comments and suggestions to improve the piece:

1-Regarding the discussion about the limits and problems associated to the measurement of political sophistication with factual political knowledge survey items, I think it is well focused and refined but there are two relevant factors that the paper does not discuss: 

a-The inclination of conventional knowledge survey items to focus on the measurement of the capacity of citizens to remember facts. This implies giving priority to their declarative memory, while leaving aside their procedural memory. At present this piece does not dialog with this relevant literature.

b-The relevance of the format (that is true/false; Multiple choice, Open ended, etc.) to obtain different estimations of peoples' levels of political sophistication. There is a debate in the literature about this question too. The paper could benefit from the main results of this debate comparing different formats across conventional factual knowledge items. There is a recent paper at SSQ that deals with this (\url{https://onlinelibrary.wiley.com/doi/full/10.1111/ssqu.12822})

2-Regarding the discussion about the potential sources of measurement bias influencing the apparent gender gap, I have also missed the relevance of format (that is true/false; Multiple choice, Open ended, etc.) and the temporal dimension of the questions. If I remember well two articles in APSR 2014 (Barabas et al) and P\&G 2018 (Ferrin et al) discuss this point. 

3-The whole idea about discursive sophistication is solidly based on a classic contribution to the field (Converse 1964 and Luskin 1987), or what is known in the literature as the structure of belief systems. However, at present I think that the contribution of the manuscript is mainly empirical. Or is there something new with respect to these two studies that the present manuscript advance? If such is the case, then the author(s) should make it explicit to convince the reader.

4-In the discussion about the definition of discursive sophistication (distinguishing three dimensions: size, range and constraint) it is not clear how each of these dimensions are linked to the concept that the author(s) intend(s) to measure: the extent to which citizens are able to understand the functioning of institutions, the performance of the incumbent government, and the actions of the main political actors. This is the only way for people to assess their interest as individuals and as members of groups. 

5-Regarding the empirics, the paper uses existing survey evidence containing batteries of open-ended questions about very different specific topics such as gun legislation, health care, immigration, preferences for party and candidates. I wonder if these are the most adequate type of topics to measure discursive sophistication, given their partisan roots. In other words: how ideology or partisanship might be affecting the main findings reported in this paper? 

The piece does discuss the univariate densities across the three indicators: discursive sophistication has always a close to normal distribution while factual knowledge and interviewer evaluations present a more heterogenous distribution. What is the implication of this result for the rest of the estimations? This needs to be discussed.

With respect to the validation of the measure in Figure 3, it is true that the estimates corresponding to discursive sophistication appear to be of a greater size than those corresponding to factual knowledge. However, it is also true that the precision of the estimates is also smaller (this is especially relevant for 2018 CES and 2016 ANES estimations)

Gender differences Figure 6 shows that average levels of discursive sophistication are systematically smaller than factual knowledge. This suggests that discursive sophistication is a quality that only those highly motivated (and perhaps partisan) citizens have. So, we have the situation of low average level of discursive sophistication with no gender differences. I wonder if this is the same for the other sources of inequality in political sophistication (such as education or age) that the extant literature has found.


\subsection*{Reviewer 3}

I’ve read this paper carefully. The text analysis methods and the structural topic model fall outside my area of expertise. However, I consider myself methodologically sophisticated and familiar with the literature on political knowledge and the gender gap.

Here’s my take on the paper: It’s wonderful. I have a couple of minor, easy-to-implement suggestions that I suspect will marginally improve the paper. But it’s wonderful as-is. My formal recommendation is minor revisions.

The paper is well-written. It has a compelling conceptual argument and a compelling empirical argument. It discusses a theoretically and normatively interesting topic. I can’t remember ever reviewing a paper I enjoyed this much—I like everything about it. I regret only that I didn’t complete it sooner, because I would have loved to let my undergraduates read the paper this semester.

\subsubsection*{The Comparability of the Effects of Discursive and Factual Knowledge}

I would like to see the concerns below addressed in a revised submission. I suspected they will require only a little work, and that any changes will improve (and not undermine) the argument. 

One small suggestion, “average marginal effect” is a bit vague. Perhaps I missed it, but I want to confirm that the effects shown in Figure 3 are comparable across discursive and factual knowledge. For example, are these the change in the probability of voting as the individual moves from the 25th to 75 percentile on each measure? If the factual and discursive measures are scaled differently, then the results are not comparable. It doesn’t seem trivial to select a comparable shift for the two measures. Even if both are scaled from 0 to 1, then the two effects might not be comparable, because a shift from ..3 to .5 might be a shift from a person with very low discursive knowledge (say, 20th percentile) to very high (say, 80th percentile), while a comparable shift in factual knowledge might be 0.2 to 1.0. See the different distributions of factual and discursive knowledge in Figure 2, for example.

Alternatively, the authors might make the measure of discursive and factual knowledge comparable by rescaling the SD of each to one. This does not guarantee a comparable effect, but I think it’s a much more plausible default than rescaling from zero to one.

The same argument applies to Figure 4. It could be that the factual measure has many observations along the entire range from 0 to 1, while almost of the discursive measures fall between .2 and .5. In this case, increasing both measures from there 20th to 80th percentiles (a comparable shift, I suggest) would produce a similar difference in retrieval.

It also isn’t clear why the authors use control variables in these models. I don’t understand the purpose of the control variables (not causal inference, see middle paragraph on p. 17), so it’s difficult to evaluate whether these are the appropriate control variables or not. The context makes me think the authors might want no control variables at all—just a simple linear or logistic regression with factual/discursive knowledge as the explanatory variable.

Rather than look at effect size at all, the authors might consider comparing the fit of the two models against each other using the AIC and/or BIC. I suspect this is the route I would have chosen if I had been trying to make a similar point.

\subsubsection*{Arguments for the Null Effect}

This is merely a suggestion for the authors. If they find my suggestion unhelpful or misguided, they should feel free to ignore it.

I also suggest that the authors think more carefully about their argument for no gender gap. Rainey (2014, AJPS, “Arguing for a Negligible Effect”) shows that a lack of statistical significance is neither necessary nor sufficient to demonstrate “no effect.” Instead, he argues that the researcher should/must argue that all the effects in the 90\% confidence interval are substantively negligible. Looking at the CIs in Figure 7, it seems that the CIs contain only very small effects, but perhaps the authors should make the explicit argument that these effects are indeed negligible.

Related to the point above, it is critical that the effects shown in Figure 7 are comparable. I suspect it’s not the case, but it’s possible that a 0.01 gap in discursive knowledge is “large,” while a 0.05 gap in factual knowledge is “small.” It might be easiest to think of effect sizes in terms of standard deviations or percentile shifts in the original outcome. For example, perhaps a 0.01 gap in discursive knowledge is a 1 SD shift or a shift from the 25th to the 75th percentile, while a 0.05 shift in factual knowledge is a 0.25 SD shift or a shift from the 45th to the 55th percentile. (Figure 21 shows the distributions, so I can almost work out—but not quite—whether these effects are comparable.)

\subsubsection*{Figure 6}

This is merely a suggestion for the authors. If they find my suggestion unhelpful or misguided, they should feel free to ignore it.

Figure 6 is awesome, and it makes a really powerful point. However, I wonder if a histogram of the entire distribution (for men and women separately, rather than showing only the average for each) would be informative. A “violin plot” or a “beeswarm” plot could show the entire distribution for men and women in a similar space. It would use the space above the error band (currently empty) and the space below the error band (filled by a not-strictly necessary bar) to show the entire distribution.

It could be that showing the entire distribution for men and women is not helpful, but I’m curious, as a reader, what it looks like.